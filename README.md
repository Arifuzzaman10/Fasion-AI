# Fasion-AI
Effective Training of Diffusion Model for Photorealistic Text-to-Image Synthesis




Contents
ğŸ‰ News
ğŸ“‘ Open-source Plan
ğŸ“– Introduction
ğŸ“Š Evaluation ğŸ¥‡ğŸ¥‡ğŸ”¥ğŸ”¥
ğŸ¥ Visualization
ğŸ› ï¸ Usage
ğŸ“œ License & Citation & Acknowledgments

ğŸ‰ News
2024.09.01 ğŸ”¥ Kolors-Virtual-Try-On, a virtual try-on demo based on Kolors is released! Enjoy trying on Kolors-Virtual-Try-On, WeChat post.

2024.08.06 ğŸ”¥ Pose ControlNet is released! Please check ControlNet(Pose) for more details.

2024.08.01 ğŸ”¥ The Kolors-Dreambooth-LoRA training and inference code is released! Please check Dreambooth-LoRA for more details.

2024.07.31 ğŸ”¥ The Kolors-IP-Adapter-FaceID-Plus weights and inference code is released! Please check IP-Adapter-FaceID-Plus for more details.

2024.07.26 ğŸ”¥ ControlNet and Inpainting Model are released! Please check ControlNet(Canny, Depth) and Inpainting Model for more details.

2024.07.17 ğŸ”¥ The Kolors-IP-Adapter-Plus weights and infernce code is released! Please check IP-Adapter-Plus for more details.

2024.07.12 ğŸ¤— Kolors is now available in Diffusers! Please check kolors-diffusers or the example below for detail! Thanks to the Diffusers team for their technical support.

2024.07.10 ğŸ¤– Kolors supports ModelScope.

2024.07.09 ğŸ’¥ Kolors supports ComfyUI. Thanks to @kijai with his great work.

2024.07.06 ğŸ”¥ğŸ”¥ğŸ”¥ We release Kolors, a large text-to-image model trained on billions of text-image pairs. This model is bilingual in both Chinese and English, and supports a context length of 256 tokens. For more technical details, please refer to technical report.

2024.07.03 ğŸ“Š Kolors won the second place on FlagEval Multimodal Text-to-Image Leaderboard, excelling particularly in the Chinese and English subjective quality assessment where Kolors took the first place.

2024.07.02 ğŸ‰ Congratulations! Our paper on controllable video generation, DragAnything: Motion Control for Anything using Entity Representation, have been accepted by ECCV 2024.

2024.02.08 ğŸ‰ Congratulations! Our paper on generative model evaluation, Learning Multi-dimensional Human Preference for Text-to-Image Generation, have been accepted by CVPR 2024.


ğŸ“‘ Open-source Plan
Kolors (Text-to-Image Model)
 Inference
 Checkpoints
 IP-Adapter
 ControlNet (Canny, Depth)
 Inpainting
 IP-Adapter-FaceID
 LoRA
 ControlNet (Pose)
 ComfyUI
 Gradio
 Diffusers

ğŸ“– Introduction
Kolors is a large-scale text-to-image generation model based on latent diffusion, developed by the Kuaishou Kolors team. Trained on billions of text-image pairs, Kolors exhibits significant advantages over both open-source and closed-source models in visual quality, complex semantic accuracy, and text rendering for both Chinese and English characters. Furthermore, Kolors supports both Chinese and English inputs, demonstrating strong performance in understanding and generating Chinese-specific content. For more details, please refer to this technical report.


ğŸ“Š Evaluation
We have collected a comprehensive text-to-image evaluation dataset named KolorsPrompts to compare Kolors with other state-of-the-art open models and closed-source models. KolorsPrompts includes over 1,000 prompts across 14 catagories and 12 evaluation dimensions. The evaluation process incorporates both human and machine assessments. In relevant benchmark evaluations, Kolors demonstrated highly competitive performance, achieving industry-leading standards.




Human Assessment
For the human evaluation, we invited 50 imagery experts to conduct comparative evaluations of the results generated by different models. The experts rated the generated images based on three criteria: visual appeal, text faithfulness, and overall satisfaction. In the evaluation, Kolors achieved the highest overall satisfaction score and significantly led in visual appeal compared to other models.

Model	Average Overall Satisfaction	Average Visual Appeal	Average Text Faithfulness
Adobe-Firefly	3.03	3.46	3.84
Stable Diffusion 3	3.26	3.50	4.20
DALL-E 3	3.32	3.54	4.22
Midjourney-v5	3.32	3.68	4.02
Playground-v2.5	3.37	3.73	4.04
Midjourney-v6	3.58	3.92	4.18
Kolors	3.59	3.99	4.17
All model results are tested with the April 2024 product versions


Machine Assessment
We used MPS (Multi-dimensional Human Preference Score) on KolorsPrompts as the evaluation metric for machine assessment. Kolors achieved the highest MPS score, which is consistent with the results of the human evaluations.

Models	Overall MPS
Adobe-Firefly	8.5
Stable Diffusion 3	8.9
DALL-E 3	9.0
Midjourney-v5	9.4
Playground-v2.5	9.8
Midjourney-v6	10.2
Kolors	10.3

For more experimental results and details, please refer to our technical report.




ğŸ¥ Visualization
High-quality Portrait


Chinese Elements Generation


Complex Semantic Understanding


Text Rendering


The visualized case prompts mentioned above can be accessed here.


ğŸ› ï¸ Usage
Requirements
Python 3.8 or later
PyTorch 1.13.1 or later
Transformers 4.26.1 or later
Recommended: CUDA 11.7 or later

Repository Cloning and Dependency Installation
apt-get install git-lfs
git clone https://github.com/Kwai-Kolors/Kolors
cd Kolors
conda create --name kolors python=3.8
conda activate kolors
pip install -r requirements.txt
python3 setup.py install
Weights downloadï¼ˆlinkï¼‰ï¼š
huggingface-cli download --resume-download Kwai-Kolors/Kolors --local-dir weights/Kolors
or

git lfs clone https://huggingface.co/Kwai-Kolors/Kolors weights/Kolors
Inferenceï¼š
python3 scripts/sample.py "ä¸€å¼ ç“¢è™«çš„ç…§ç‰‡ï¼Œå¾®è·ï¼Œå˜ç„¦ï¼Œé«˜è´¨é‡ï¼Œç”µå½±ï¼Œæ‹¿ç€ä¸€ä¸ªç‰Œå­ï¼Œå†™ç€â€œå¯å›¾â€"
# The image will be saved to "scripts/outputs/sample_text.jpg"
Web demoï¼š
python3 scripts/sampleui.py
Using with Diffusers
Make sure you upgrade to the latest version(0.30.0.dev0) of diffusers:

git clone https://github.com/huggingface/diffusers
cd diffusers
python3 setup.py install
Notes:

The pipeline uses the EulerDiscreteScheduler by default. We recommend using this scheduler with guidance scale=5.0 and num_inference_steps=50.
The pipeline also supports the EDMDPMSolverMultistepScheduler. guidance scale=5.0 and num_inference_steps=25 is a good default for this scheduler.
In addition to Text-to-Image, KolorsImg2ImgPipeline also supports Image-to-Image.
And then you can run:

import torch
from diffusers import KolorsPipeline
pipe = KolorsPipeline.from_pretrained(
    "Kwai-Kolors/Kolors-diffusers", 
    torch_dtype=torch.float16, 
    variant="fp16"
).to("cuda")
prompt = 'ä¸€å¼ ç“¢è™«çš„ç…§ç‰‡ï¼Œå¾®è·ï¼Œå˜ç„¦ï¼Œé«˜è´¨é‡ï¼Œç”µå½±ï¼Œæ‹¿ç€ä¸€ä¸ªç‰Œå­ï¼Œå†™ç€"å¯å›¾"'
image = pipe(
    prompt=prompt,
    negative_prompt="",
    guidance_scale=5.0,
    num_inference_steps=50,
    generator=torch.Generator(pipe.device).manual_seed(66),
).images[0]
image.show()
IP-Adapter-Plus
We provide IP-Adapter-Plus weights and inference code, detailed in the ipadapter.

# Weights download
huggingface-cli download --resume-download Kwai-Kolors/Kolors-IP-Adapter-Plus --local-dir weights/Kolors-IP-Adapter-Plus
# Inferenceï¼š
python3 ipadapter/sample_ipadapter_plus.py ./ipadapter/asset/test_ip.jpg "ç©¿ç€é»‘è‰²Tæ¤è¡«ï¼Œä¸Šé¢ä¸­æ–‡ç»¿è‰²å¤§å­—å†™ç€â€œå¯å›¾â€"

python3 ipadapter/sample_ipadapter_plus.py ./ipadapter/asset/test_ip2.png "ä¸€åªå¯çˆ±çš„å°ç‹—åœ¨å¥”è·‘"

# The image will be saved to "scripts/outputs/"
ControlNet
We provide three ControlNet weights and inference code, detailed in the controlnet.

# Weights download

# Canny - ControlNet
huggingface-cli download --resume-download Kwai-Kolors/Kolors-ControlNet-Canny --local-dir weights/Kolors-ControlNet-Canny

# Depth - ControlNet
huggingface-cli download --resume-download Kwai-Kolors/Kolors-ControlNet-Depth --local-dir weights/Kolors-ControlNet-Depth

# Pose - ControlNet
huggingface-cli download --resume-download Kwai-Kolors/Kolors-ControlNet-Pose --local-dir weights/Kolors-ControlNet-Pose
If you intend to utilize the depth estimation network, please make sure to download its corresponding model weights.

huggingface-cli download lllyasviel/Annotators ./dpt_hybrid-midas-501f0c75.pt --local-dir ./controlnet/annotator/ckpts
Thanks to DWPose, you can utilize the pose estimation network. Please download the Pose model dw-ll_ucoco_384.onnx (baidu, google) and Det model yolox_l.onnx (baidu, google). Then please put them into controlnet/annotator/ckpts/.

# Inferenceï¼š

python ./controlnet/sample_controlNet.py ./controlnet/assets/woman_1.png ä¸€ä¸ªæ¼‚äº®çš„å¥³å­©ï¼Œé«˜å“è´¨ï¼Œè¶…æ¸…æ™°ï¼Œè‰²å½©é²œè‰³ï¼Œè¶…é«˜åˆ†è¾¨ç‡ï¼Œæœ€ä½³å“è´¨ï¼Œ8kï¼Œé«˜æ¸…ï¼Œ4K Canny

python ./controlnet/sample_controlNet.py ./controlnet/assets/woman_2.png æ–°æµ·è¯šé£æ ¼ï¼Œä¸°å¯Œçš„è‰²å½©ï¼Œç©¿ç€ç»¿è‰²è¡¬è¡«çš„å¥³äººç«™åœ¨ç”°é‡é‡Œï¼Œå”¯ç¾é£æ™¯ï¼Œæ¸…æ–°æ˜äº®ï¼Œæ–‘é©³çš„å…‰å½±ï¼Œæœ€å¥½çš„è´¨é‡ï¼Œè¶…ç»†èŠ‚ï¼Œ8Kç”»è´¨ Depth

python ./controlnet/sample_controlNet.py ./controlnet/assets/woman_3.png ä¸€ä½ç©¿ç€ç´«è‰²æ³¡æ³¡è¢–è¿è¡£è£™ã€æˆ´ç€çš‡å† å’Œç™½è‰²è•¾ä¸æ‰‹å¥—çš„å¥³å­©åŒæ‰‹æ‰˜è„¸ï¼Œé«˜å“è´¨ï¼Œè¶…æ¸…æ™°ï¼Œè‰²å½©é²œè‰³ï¼Œè¶…é«˜åˆ†è¾¨ç‡ï¼Œæœ€ä½³å“è´¨ï¼Œ8kï¼Œé«˜æ¸…ï¼Œ4K Pose

# The image will be saved to "controlnet/outputs/"
Inpainting
We provide Inpainting weights and inference code, detailed in the inpainting.

# Weights download
huggingface-cli download --resume-download Kwai-Kolors/Kolors-Inpainting --local-dir weights/Kolors-Inpainting
# Inferenceï¼š
python3 inpainting/sample_inpainting.py ./inpainting/asset/3.png ./inpainting/asset/3_mask.png ç©¿ç€ç¾å°‘å¥³æˆ˜å£«çš„è¡£æœï¼Œä¸€ä»¶ç±»ä¼¼äºæ°´æ‰‹æœé£æ ¼çš„è¡£æœï¼ŒåŒ…æ‹¬ä¸€ä¸ªç™½è‰²ç´§èº«ä¸Šè¡£ï¼Œå‰èƒ¸æ­é…ä¸€ä¸ªå¤§å¤§çš„çº¢è‰²è´è¶ç»“ã€‚è¡£æœçš„é¢†å­éƒ¨åˆ†å‘ˆè“è‰²ï¼Œå¹¶ä¸”æœ‰ç™½è‰²æ¡çº¹ã€‚å¥¹è¿˜ç©¿ç€ä¸€æ¡è“è‰²ç™¾è¤¶è£™ï¼Œè¶…é«˜æ¸…ï¼Œè¾›çƒ·æ¸²æŸ“ï¼Œé«˜çº§è´¨æ„Ÿï¼Œ32kï¼Œé«˜åˆ†è¾¨ç‡ï¼Œæœ€å¥½çš„è´¨é‡ï¼Œè¶…çº§ç»†èŠ‚ï¼Œæ™¯æ·±

python3 inpainting/sample_inpainting.py ./inpainting/asset/4.png ./inpainting/asset/4_mask.png ç©¿ç€é’¢é“ä¾ çš„è¡£æœï¼Œé«˜ç§‘æŠ€ç›”ç”²ï¼Œä¸»è¦é¢œè‰²ä¸ºçº¢è‰²å’Œé‡‘è‰²ï¼Œå¹¶ä¸”æœ‰ä¸€äº›é“¶è‰²è£…é¥°ã€‚èƒ¸å‰æœ‰ä¸€ä¸ªäº®èµ·çš„åœ†å½¢ååº”å †è£…ç½®ï¼Œå……æ»¡äº†æœªæ¥ç§‘æŠ€æ„Ÿã€‚è¶…æ¸…æ™°ï¼Œé«˜è´¨é‡ï¼Œè¶…é€¼çœŸï¼Œé«˜åˆ†è¾¨ç‡ï¼Œæœ€å¥½çš„è´¨é‡ï¼Œè¶…çº§ç»†èŠ‚ï¼Œæ™¯æ·±

# The image will be saved to "scripts/outputs/"
IP-Adapter-FaceID-Plus
We provide IP-Adapter-FaceID-Plus weights and inference code, detailed in the ipadapter_FaceID.

# Weights download
huggingface-cli download --resume-download Kwai-Kolors/Kolors-IP-Adapter-FaceID-Plus --local-dir weights/Kolors-IP-Adapter-FaceID-Plus
# Inferenceï¼š
python ipadapter_FaceID/sample_ipadapter_faceid_plus.py ./ipadapter_FaceID/assets/image1.png "ç©¿ç€æ™šç¤¼æœï¼Œåœ¨æ˜Ÿå…‰ä¸‹çš„æ™šå®´åœºæ™¯ä¸­ï¼Œçƒ›å…‰é—ªé—ªï¼Œæ•´ä¸ªåœºæ™¯æ´‹æº¢ç€æµªæ¼«è€Œå¥¢åçš„æ°›å›´"

python ipadapter_FaceID/sample_ipadapter_faceid_plus.py ./ipadapter_FaceID/assets/image2.png "è¥¿éƒ¨ç‰›ä»”ï¼Œç‰›ä»”å¸½ï¼Œè’é‡å¤§é•–å®¢ï¼ŒèƒŒæ™¯æ˜¯è¥¿éƒ¨å°é•‡ï¼Œä»™äººæŒï¼Œ,æ—¥è½ä½™æ™–, æš–è‰²è°ƒ, ä½¿ç”¨XT4èƒ¶ç‰‡æ‹æ‘„, å™ªç‚¹, æ™•å½±, æŸ¯è¾¾èƒ¶å·ï¼Œå¤å¤"

# The image will be saved to "scripts/outputs/"
Dreambooth-LoRA
We provide LoRA training and inference code, detailed in the Dreambooth-LoRA.

# Training:
sh train.sh
# Inferenceï¼š
python infer_dreambooth.py "ktxlç‹—åœ¨è‰åœ°ä¸Šè·‘"



ğŸ“œ License & Citation & Acknowledgments
License
Kolors weights are fully open for academic research. If you intend to use the Kolors model or its derivatives for commercial purposes under the licensing terms and conditions, please send the questionnaire to kwai-kolors@kuaishou.com to register with the licensor. If the monthly active users of all products or services made available by or for Licensee does not exceed 300 million monthly active users in the preceding calendar month, Your registration with the Licensor will be deemed to have obtained the corresponding business license; If, the monthly active users of all products or services made available by or for Licensee is greater than 300 million monthly active users in the preceding calendar month, You must request a license from Licensor, which the Licensor may grant to You in its sole discretion, and You are not authorized to exercise any of the rights under this Agreement unless or until We otherwise expressly grants You such rights.

We open-source Kolors to promote the development of large text-to-image models in collaboration with the open-source community. The code of this project is open-sourced under the Apache-2.0 license. We sincerely urge all developers and users to strictly adhere to the open-source license, avoiding the use of the open-source model, code, and its derivatives for any purposes that may harm the country and society or for any services not evaluated and registered for safety. Note that despite our best efforts to ensure the compliance, accuracy, and safety of the data during training, due to the diversity and combinability of generated content and the probabilistic randomness affecting the model, we cannot guarantee the accuracy and safety of the output content, and the model is susceptible to misleading. This project does not assume any legal responsibility for any data security issues, public opinion risks, or risks and liabilities arising from the model being misled, abused, misused, or improperly utilized due to the use of the open-source model and code.

Citation
If you find our work helpful, please cite it!

@article{kolors,
  title={Kolors: Effective Training of Diffusion Model for Photorealistic Text-to-Image Synthesis},
  author={Kolors Team},
  journal={arXiv preprint},
  year={2024}
}
Acknowledgments
Thanks to Diffusers for providing the codebase.
Thanks to ChatGLM3 for providing the powerful Chinese language model.

Contact Us
If you want to leave a message for our R&D team and product team, feel free to join our WeChat group. You can also contact us via email (kwai-kolors@kuaishou.com).

Star History Chart
